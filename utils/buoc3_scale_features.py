# utils/buoc3_scale_features.py
# ------------------------------
# BÆ¯á»šC 3: Scale features (train-only), log-transform label
# - Input  : data/processed/weekly_pt/*.pt  (táº¡o tá»« BÆ°á»›c 2)
# - Output : data/processed/weekly_pt_scaled/*.pt  (x Ä‘Ã£ scale, y=log1p(casos))
# - Meta   : data/processed/scaler_weekly.json  (tham sá»‘ scaler theo feature)
#
# Ghi chÃº:
# - KhÃ´ng dÃ¹ng 'date' (Ä‘Ã£ bá» á»Ÿ BÆ°á»›c 2). Chá»‰ dá»±a vÃ o year+epiweek.
# - Scaler Ä‘Æ°á»£c Æ°á»›c lÆ°á»£ng CHá»ˆ tá»« nÄƒm train (2010-2020) Ä‘á»ƒ trÃ¡nh leakage.
# - Há»— trá»£ 'standard' (máº·c Ä‘á»‹nh) hoáº·c 'minmax'.
# - Label: y' = log1p(y). KhÃ´ng scale thÃªm.
# - Giá»¯ nguyÃªn thá»© tá»± node/feature_cols/masks; chá»‰ thay x vÃ  y.

import os
import json
import glob
import math
import torch
import numpy as np

PROCESSED_DIR = "data/processed"
SRC_DIR = os.path.join(PROCESSED_DIR, "weekly_pt")
DST_DIR = os.path.join(PROCESSED_DIR, "weekly_pt_scaled")
SCALER_JSON = os.path.join(PROCESSED_DIR, "scaler_weekly.json")

os.makedirs(DST_DIR, exist_ok=True)

# ------------------------------
# Cáº¥u hÃ¬nh scaler
# ------------------------------
SCALER_TYPE = "standard"  # "standard" | "minmax"
EPS = 1e-6

def is_train_year(y: int) -> bool:
    return 2010 <= y <= 2020

def parse_year_week_from_fname(fname: str):
    # vÃ­ dá»¥: .../2017_14.pt hoáº·c 2017_14_something.pt
    base = os.path.basename(fname)
    name, _ = os.path.splitext(base)
    parts = name.split("_")
    if len(parts) < 2:
        return None, None
    try:
        yy = int(parts[0])
        ww = int(parts[1])
        return yy, ww
    except:
        return None, None

# ------------------------------
# 1) QuÃ©t file, Ä‘á»c meta Ä‘á»ƒ láº¥y sá»‘ feature & feature_cols
# ------------------------------
pt_files = sorted(glob.glob(os.path.join(SRC_DIR, "*.pt")))
if not pt_files:
    raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y snapshot .pt trong {SRC_DIR}. HÃ£y cháº¡y BÆ°á»›c 2 trÆ°á»›c.")

# Láº¥y má»™t file máº«u Ä‘á»ƒ biáº¿t feature_cols
sample = torch.load(pt_files[0], map_location="cpu", weights_only=False)
feature_cols = sample.get("feature_cols", None)
if feature_cols is None:
    raise KeyError("Thiáº¿u 'feature_cols' trong snapshot .pt (BÆ°á»›c 2).")

F = len(feature_cols)
print(f"ğŸ“¦ BÆ°á»›c 3 â€” Scale features ({SCALER_TYPE}), log1p(y). Sá»‘ feature: {F}")

# ------------------------------
# 2) Æ¯á»›c lÆ°á»£ng tham sá»‘ scaler tá»« TRAIN (2010-2020)
#    - Standard: mean, std (per-feature)
#    - MinMax  : min, max (per-feature)
# ------------------------------
if SCALER_TYPE == "standard":
    # sum, sumsq, count Ä‘á»ƒ tÃ­nh mean/std á»•n Ä‘á»‹nh
    sum_feat = np.zeros((F,), dtype=np.float64)
    sumsq_feat = np.zeros((F,), dtype=np.float64)
    count = 0
elif SCALER_TYPE == "minmax":
    min_feat = np.full((F,), np.inf, dtype=np.float64)
    max_feat = np.full((F,), -np.inf, dtype=np.float64)
else:
    raise ValueError("SCALER_TYPE pháº£i lÃ  'standard' hoáº·c 'minmax'.")

train_files = []
for p in pt_files:
    yy, ww = parse_year_week_from_fname(p)
    if yy is None:
        continue
    if is_train_year(yy):
        train_files.append(p)

if not train_files:
    raise RuntimeError("KhÃ´ng tÃ¬m tháº¥y file TRAIN (2010-2020) Ä‘á»ƒ Æ°á»›c lÆ°á»£ng scaler.")

for p in train_files:
    d = torch.load(p, map_location="cpu", weights_only=False)
    x = d["x"].cpu().numpy().astype(np.float64)  # (N,F)
    # báº£o Ä‘áº£m shape há»£p lá»‡
    if x.ndim != 2 or x.shape[1] != F:
        raise ValueError(f"x shape khÃ´ng khá»›p sá»‘ feature á»Ÿ {p}: {x.shape} vs F={F}")

    if SCALER_TYPE == "standard":
        sum_feat += x.sum(axis=0)
        sumsq_feat += (x * x).sum(axis=0)
        count += x.shape[0]
    elif SCALER_TYPE == "minmax":
        min_feat = np.minimum(min_feat, np.nanmin(x, axis=0))
        max_feat = np.maximum(max_feat, np.nanmax(x, axis=0))

if SCALER_TYPE == "standard":
    mean = sum_feat / max(count, 1)
    var = (sumsq_feat / max(count, 1)) - (mean * mean)
    var = np.maximum(var, 0.0)
    std = np.sqrt(var)
    std = np.where(std < EPS, 1.0, std)  # trÃ¡nh chia 0
    scaler_params = {"type": "standard",
                     "mean": mean.tolist(),
                     "std": std.tolist(),
                     "feature_cols": feature_cols,
                     "train_count_rows": int(count)}
elif SCALER_TYPE == "minmax":
    # trÃ¡nh TH min == max
    span = max_feat - min_feat
    span = np.where(span < EPS, 1.0, span)
    scaler_params = {"type": "minmax",
                     "min": min_feat.tolist(),
                     "max": max_feat.tolist(),
                     "feature_cols": feature_cols}

with open(SCALER_JSON, "w", encoding="utf-8") as f:
    json.dump(scaler_params, f, ensure_ascii=False, indent=2)
print(f"âœ… LÆ°u tham sá»‘ scaler: {SCALER_JSON}")

# ------------------------------
# 3) Ãp dá»¥ng scaler cho Má»ŒI snapshot vÃ  log1p cho y
#    - Ghi ra thÆ° má»¥c weekly_pt_scaled
# ------------------------------
n_written = 0

for p in pt_files:
    d = torch.load(p, map_location="cpu", weights_only=False)

    x = d["x"].cpu().numpy().astype(np.float32)  # (N,F)
    y = d["y"].cpu().numpy().astype(np.float32)  # (N,)

    # scale X
    if SCALER_TYPE == "standard":
        mean = np.array(scaler_params["mean"], dtype=np.float32)
        std  = np.array(scaler_params["std"], dtype=np.float32)
        x_scaled = (x - mean) / std
    else:  # minmax
        _min = np.array(scaler_params["min"], dtype=np.float32)
        _max = np.array(scaler_params["max"], dtype=np.float32)
        span = _max - _min
        span = np.where(span < EPS, 1.0, span).astype(np.float32)
        x_scaled = (x - _min) / span  # in [0,1]

    # log1p cho y
    y_log = np.log1p(np.maximum(y, 0.0, dtype=np.float32))

    # táº¡o báº£n sao dict Ä‘á»ƒ ghi
    out = dict(d)
    out["x"] = torch.tensor(x_scaled, dtype=torch.float32)
    out["y"] = torch.tensor(y_log, dtype=torch.float32)
    out["label_transform"] = "log1p"
    out["scaler"] = scaler_params

    fname = os.path.basename(p)
    torch.save(out, os.path.join(DST_DIR, fname))
    n_written += 1

print(f"âœ… ÄÃ£ scale & lÆ°u {n_written} snapshot @ {DST_DIR}")

# ------------------------------
# 4) Kiá»ƒm tra nhanh 1 file máº«u
# ------------------------------
sample_out = os.path.join(DST_DIR, os.path.basename(pt_files[0]))
chk = torch.load(sample_out, map_location="cpu", weights_only=False)
xs = chk["x"].numpy()
ys = chk["y"].numpy()

# sanity
nan_x = np.isnan(xs).sum()
inf_x = np.isinf(xs).sum()
nan_y = np.isnan(ys).sum()
inf_y = np.isinf(ys).sum()

print("Sanity (sample):")
print(f"  x shape={xs.shape}, NaN={nan_x}, Inf={inf_x}")
print(f"  y shape={ys.shape}, NaN={nan_y}, Inf={inf_y}")
print(f"  feature_cols={len(chk.get('feature_cols', []))}, label_transform={chk.get('label_transform')}")
print("ğŸ‰ BÆ°á»›c 3 hoÃ n táº¥t.")
